{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install transformers datasets peft evaluate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fc6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "pip install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2416f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c063298-9945-4c00-8c98-0253e10116f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow==15.0.2\n",
      "  Downloading pyarrow-15.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in /Users/chrismo/opt/anaconda3/lib/python3.11/site-packages (from pyarrow==15.0.2) (1.26.4)\n",
      "Downloading pyarrow-15.0.2-cp311-cp311-macosx_11_0_arm64.whl (24.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 17.0.0\n",
      "    Uninstalling pyarrow-17.0.0:\n",
      "      Successfully uninstalled pyarrow-17.0.0\n",
      "Successfully installed pyarrow-15.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow==15.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d86fd96a-2cd2-4210-a07e-78a109419a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow==15.0.2 in /Users/chrismo/opt/anaconda3/lib/python3.11/site-packages (15.0.2)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in /Users/chrismo/opt/anaconda3/lib/python3.11/site-packages (from pyarrow==15.0.2) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow==15.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65cf86d1-fe70-4be7-8156-e5afbe0e4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "conda install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1869679-0249-457b-bae8-30c24e86484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyarrow 15.0.2\n",
      "Uninstalling pyarrow-15.0.2:\n",
      "  Would remove:\n",
      "    /Users/chrismo/opt/anaconda3/lib/python3.11/site-packages/pyarrow-15.0.2.dist-info/*\n",
      "    /Users/chrismo/opt/anaconda3/lib/python3.11/site-packages/pyarrow/*\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "pip uninstall pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6be751-448e-43d8-8200-7c4491728b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fec44373",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow.lib' has no attribute 'ListViewType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score, accuracy_score\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/datasets/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:76\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_reader.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, List, Optional, Union\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/pyarrow/parquet/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# or more contributor license agreements.  See the NOTICE file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# distributed with this work for additional information\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/pyarrow/parquet/core.py:33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_parquet\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pyarrow installation is not built with support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the Parquet file format (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(exc)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/pyarrow/_parquet.pyx:1\u001b[0m, in \u001b[0;36minit pyarrow._parquet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow.lib' has no attribute 'ListViewType'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf6d46",
   "metadata": {},
   "source": [
    "#### using an uncased model as they always perform better in most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7388150",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'allenai/scibert_scivocab_uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935cb4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  \n",
    "#num_labels = 2 meaning we wake it a yes or no kinda scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac076ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223561f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"deepmind/math_dataset\", \"calculus__differentiate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,   #let us use 16 rank\n",
    "    lora_alpha=32,   #scaling factor\n",
    "    target_modules=[\"query\", \"value\"],          #specific layers to apply lora to\n",
    "    lora_dropout=0.05,     \n",
    "    bias=\"none\",     #bias type for LoRA\n",
    "    task_type=\"SEQ_CLS\"   #sequence classification task, simply denoting correct or incorrect for a question\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84749b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)   #specifically applying LoRA to our SCIBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extracting questions from the input examples and then using tokenizer to process them\n",
    "    \n",
    "    Arg: examples(dict) = dictionary containing batvch of examples\n",
    "    \n",
    "    Returns: labels(dict) = dictionary containing tokenized values to explain inputs_id and attention mask\n",
    "    \n",
    "    \"\"\"\n",
    "    questions = examples[\"question\"]\n",
    "    inputs = tokenizer(questions, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs[\"labels\"] = [1 if answer == \"correct\" else 0 for answer in examples[\"answer\"]]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_function??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0], dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessed_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f124f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator =  DataCollatorWithPadding(tokenizer=tokenizer, padding=True)   #ensuring consistent, dynamic batch padding when training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f884f",
   "metadata": {},
   "source": [
    "### Defining training arguments now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f170cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {'f1': f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f04663",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"calculus_lora_scibert\",    #specifying where to save our model\n",
    "    num_train_epochs=1,    #number of training epochs done\n",
    "    per_device_train_batch_size=16,     #gpu processes 16 samples during training\n",
    "    per_device_eval_batch_size=16,       #process 16 samples during evaluation\n",
    "    gradient_accumulation_steps=2,     #number of forward passes before backpropagation by the per_device_train_batch_size\n",
    "    learning_rate=1e-4,       #step size taken during learning\n",
    "    fp16=True,        # memory efficiency, helps speed entire process\n",
    "    logging_steps=50,   #how often metrics are logged\n",
    "    save_steps=100,  #model chcekpoint set to 100\n",
    "    evaluation_strategy=\"epoch\"    #explains how often we evaluate our model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c726626",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_datasets[\"train\"],\n",
    "    eval_dataset=preprocessed_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad078f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17754aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our fine-tuned model\n",
    "save_directory = \"calculus_lora_scibert_saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d543a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model saved to: {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i have to type here every hour so it doesnt time out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
